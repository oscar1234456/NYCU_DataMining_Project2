{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7177ce6",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a0229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c154eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247897e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47a322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "417256d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e13589",
   "metadata": {},
   "source": [
    "# Function(Utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98060bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Duplicate file\n",
    "def checkDuplicateFile(file_path):\n",
    "    import os\n",
    "    if os.path.isfile(file_path):\n",
    "        print(\"Caution: File existed!\")\n",
    "        ans = input(\"Do you want to cover it?(Y/others)\")\n",
    "        if ans == \"Y\":\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Canceled....\")\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bcdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(clf, p_grid, X_train, y_train, X_test, y_test, run_name=\"Test\"):\n",
    "    # clf: model (classifier)\n",
    "    # p_grid: search space\n",
    "    # X_train: training data\n",
    "    # y_train: training data(target)\n",
    "    # X_test: testing data\n",
    "    # y_test: testing data(target)\n",
    "    \n",
    "    #########\n",
    "    ##wandb##\n",
    "    #########\n",
    "    wandb.init(project=\"DataMining_Project2\", entity=\"oscarchencs10\")\n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    \n",
    "    pipe = Pipeline(steps=[\n",
    "        ('model', clf)]\n",
    "    )\n",
    "\n",
    "    #採用F1-Score最高為標準\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipe, param_grid=p_grid, cv=5, n_jobs=12, scoring='f1', refit=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    print('\\nBest estimator')\n",
    "    print(grid_search.best_estimator_)\n",
    "    # print(grid_search.cv_results_)\n",
    "\n",
    "    #############\n",
    "    ### train ###\n",
    "    #############\n",
    "    print('\\n\\ntrain')\n",
    "    y_pred = grid_search.predict(X_train)\n",
    "    report = classification_report(y_train, y_pred, labels=[\n",
    "                                   1, 0], output_dict=True)  # , target_names=['0', '1']\n",
    "    acc = report.pop('accuracy')\n",
    "    target_f1_score = report['1']['f1-score']\n",
    "    y_pred_prob = grid_search.predict_proba(X_train)\n",
    "    auc = roc_auc_score(y_train, y_pred_prob[:, 1])\n",
    "\n",
    "    print(confusion_matrix(y_train, y_pred, labels=[1, 0]))\n",
    "    display(pd.DataFrame(report).T)\n",
    "    print(\n",
    "        f'Accuracy: {acc:.3f}, AUC: {auc:.3f}, f1-score: {target_f1_score:.3f} \\n\\n')\n",
    "    \n",
    "    #<wandb>#\n",
    "    wandb.sklearn.plot_classifier(grid_search, X_train, X_test, y_train, y_test, y_pred, \n",
    "                                  y_pred_prob, labels=[\"infected\",\"non-infected\"], model_name='SVC', feature_names=None)\n",
    "\n",
    "    ############\n",
    "    ### test ###\n",
    "    ############\n",
    "    print('test')\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, labels=[\n",
    "                                   1, 0], output_dict=True)  # , target_names=['0', '1']\n",
    "    acc = report.pop('accuracy')\n",
    "    target_f1_score = report['1']['f1-score']\n",
    "    y_pred_prob = grid_search.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred_prob[:, 1])\n",
    "\n",
    "    print(confusion_matrix(y_test, y_pred, labels=[1, 0]))\n",
    "    display(pd.DataFrame(report).T)\n",
    "    print(\n",
    "        f'Accuracy: {acc:.3f}, AUC: {auc:.3f}, f1-score: {target_f1_score:.3f} \\n\\n')\n",
    "\n",
    "    if grid_search.best_estimator_.steps[1][1].__class__.__name__ == 'RandomForestClassifier':\n",
    "        name = 'RandomForest (dep=' + str(grid_search.best_params_['model__max_depth']) + \\\n",
    "            ' feature=' + \\\n",
    "            str(grid_search.best_params_['model__max_features']) + ')'\n",
    "    elif grid_search.best_estimator_.steps[1][1].__class__.__name__ == 'KNeighborsClassifier':\n",
    "        name = 'KNeighbors (k=' + \\\n",
    "            str(grid_search.best_params_['model__n_neighbors']) + ')'\n",
    "    elif grid_search.best_estimator_.steps[1][1].__class__.__name__ == 'LogisticRegression':\n",
    "        name = 'LogisticRegression (penalty=' + \\\n",
    "            grid_search.best_params_['model__penalty']\n",
    "        if len(grid_search.best_params_) == 2:\n",
    "            name = name + ' solver=' + \\\n",
    "                grid_search.best_params_['model__solver']\n",
    "        name = name + ')'\n",
    "    else:\n",
    "        name = ''\n",
    "\n",
    "    return grid_search, name, acc, target_f1_score, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21341d2c",
   "metadata": {},
   "source": [
    "# Function(Use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "451576c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for checking ICU_id missing in Lab_1103_csv\n",
    "def getMissingIDinLab(Lab_file, show=True):\n",
    "    test = sorted(Lab_file.ICU_id.unique())\n",
    "    s = 0\n",
    "    error_list = list()\n",
    "    for i in test:\n",
    "        #         print(i)\n",
    "        s += 1\n",
    "        if s != i:\n",
    "            if show:\n",
    "                print(f\"error! : {s}\")\n",
    "            error_list.append(s)\n",
    "            s += 1\n",
    "    if show:\n",
    "        print(f\"Missing ID Result: {error_list}\")\n",
    "    if show:\n",
    "        print(f\"Missing Length:{len(error_list)}\")\n",
    "    return error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47ef5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store Dataframe to CSV\n",
    "def store2CSV(data, target_name, target_loc_prefix='./'):\n",
    "    file_path = target_loc_prefix+target_name+\".csv\"\n",
    "    if checkDuplicateFile(file_path):\n",
    "        print(\"store2CSV failed\")\n",
    "        return\n",
    "    data.to_csv(file_path)\n",
    "    print(\"store2CSV Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2884c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store Datastruc. to pickle\n",
    "def store2Pickle(data, target_name, target_loc_prefix='./'):\n",
    "    import pickle\n",
    "    file_path = target_loc_prefix+target_name+'.pickle'\n",
    "    if checkDuplicateFile(file_path):\n",
    "        print(\"store2Pickle failed\")\n",
    "        return\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"store2Pickle Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f016e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFPickle(target_name, target_loc_prefix='./'):\n",
    "    import pickle\n",
    "    file_name = target_loc_prefix+target_name+'.pickle'\n",
    "    with open(file_name, 'rb') as f:\n",
    "        temp = pickle.load(f)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc7929",
   "metadata": {},
   "source": [
    "# Function(Data preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b240886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: 補值\n",
    "# 將針對輸入的df_data直接進行inplace插補\n",
    "# 須確保df_data的缺失值位置有放np.nan\n",
    "def handleMissing(df_data, df_feature, outFeature=[\"outcome\"], cate_astype = \"int\"):\n",
    "    for featureName in df_data.columns:\n",
    "        if featureName not in outFeature:\n",
    "            if df_data[featureName].isna().sum() == 0:\n",
    "                print(f\"{featureName}: Not need to fill.\")\n",
    "                continue\n",
    "            else:\n",
    "                # 先去看是連續與否 (1代表連續,0代表離散)\n",
    "                kindValue = df_feature.loc[df_feature[\"features name\"]\n",
    "                                           == featureName, \"kind\"].values[0]\n",
    "                if kindValue == 1:\n",
    "                    # continuous\n",
    "                    # mean filling\n",
    "                    targetMean = df_data[featureName].mean()\n",
    "                    df_data[featureName].fillna(value=targetMean, inplace=True)\n",
    "                    print(f\"{featureName}: Fill, Continuous.\")\n",
    "\n",
    "                else:\n",
    "                    # categorical\n",
    "                    # mode filling\n",
    "                    targetMode = df_data[featureName].mode()[0]\n",
    "                    df_data[featureName].fillna(value=targetMode, inplace=True)\n",
    "                    df_data[featureName] = df_data[featureName].astype(cate_astype)\n",
    "                    print(f\"{featureName}: Fill, Categorical. (astype to {cate_astype})\")\n",
    "            \n",
    "    print(\"---handleMissing Finish---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8b46db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hist\n",
    "# filtered_data need to check not have nan\n",
    "def plotHist(df_data, target, outcome=\"outcome\", bins=20):\n",
    "    filtered_data = pd.concat([df_data[\"outcome\"], df_data[target]], axis=1)\n",
    "    filtered_data = filtered_data.dropna()\n",
    "    print(filtered_data.isna().sum())\n",
    "    plt.hist(filtered_data.loc[filtered_data[outcome]==0, target], bins=bins, alpha=0.5, label='0')\n",
    "    plt.hist(filtered_data.loc[filtered_data[outcome]==1, target], bins=bins, alpha=0.5, label='1')\n",
    "    plt.xlabel(target)\n",
    "    plt.ylabel('count')\n",
    "    plt.legend(title=outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac897349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot countplot\n",
    "# filtered_data need to check not have nan\n",
    "def plotCountplot(df_data, target, outcome=\"outcome\"):\n",
    "    filtered_data = pd.concat([df_data[\"outcome\"], df_data[target]], axis=1)\n",
    "    filtered_data = filtered_data.dropna()\n",
    "    print(filtered_data.isna().sum())\n",
    "    sns.countplot(x=target, hue=outcome, data=filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c9b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplot\n",
    "# filtered_data need to check not have nan\n",
    "def plotBoxplot(df_data, target, outcome=\"outcome\"):\n",
    "    filtered_data = pd.concat([df_data[\"outcome\"], df_data[target]], axis=1)\n",
    "    filtered_data = filtered_data.dropna()\n",
    "    print(filtered_data.isna().sum())\n",
    "    sns.boxplot(x=filtered_data[target], data=filtered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5011a7d",
   "metadata": {},
   "source": [
    "# Load Data(From Project1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2556c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f3f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2081065",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grid_rf = {'model__max_depth': [i for i in range(5, 10)],\n",
    "             'model__max_features': [i for i in range(20, 30)]}\n",
    "\n",
    "p_grid_rf_2 = {'model__max_depth': [i for i in range(1, 20)],\n",
    "               'model__max_features': [i for i in range(5, 15)]}\n",
    "\n",
    "p_grid_rf_3 = {'model__max_depth': [i for i in range(5, 25)],\n",
    "               'model__max_features': [i for i in range(10, 20)]}\n",
    "\n",
    "p_grid_knn = {'model__n_neighbors': [i for i in range(1, 15)]}\n",
    "\n",
    "p_grid_lr_1 = {'model__penalty':['l1', 'l2']}\n",
    "\n",
    "p_grid_lr_2 = {'model__penalty':['l1'], 'model__solver':['liblinear', 'saga']}\n",
    "\n",
    "p_grid_lr_3 = {'model__penalty':['l2'], 'model__solver':['newton-cg', 'lbfgs', 'sag', 'saga']}\n",
    "\n",
    "base_lines = [\n",
    "    gridsearch(KNeighborsClassifier(), p_grid_knn),\n",
    "    \n",
    "    gridsearch(RandomForestClassifier(random_state=42), p_grid_rf),\n",
    "    gridsearch(RandomForestClassifier(random_state=42), p_grid_rf_2),\n",
    "    gridsearch(RandomForestClassifier(random_state=42), p_grid_rf_3),\n",
    "    \n",
    "    gridsearch(LogisticRegression(random_state=42), p_grid_lr_1),\n",
    "    gridsearch(LogisticRegression(random_state=42), p_grid_lr_2),\n",
    "    gridsearch(LogisticRegression(random_state=42), p_grid_lr_3),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be344bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moscarchencs10\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/oscarchencs10/DataMining_Project2/runs/cz14eufc\" target=\"_blank\">fragrant-dream-1</a></strong> to <a href=\"https://wandb.ai/oscarchencs10/DataMining_Project2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"DataMining_Project2\", entity=\"oscarchencs10\")\n",
    "wandb.run.name = \"test1\"\n",
    "wandb.run.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
