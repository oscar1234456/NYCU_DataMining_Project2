{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61fd06f",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b51422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d888d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83baf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00aa74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78feca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538be6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32bdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de46b82",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea797419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function(Utility)\n",
    "\n",
    "# check Duplicate file\n",
    "def checkDuplicateFile(file_path):\n",
    "    import os\n",
    "    if os.path.isfile(file_path):\n",
    "        print(\"Caution: File existed!\")\n",
    "        ans = input(\"Do you want to cover it?(Y/others)\")\n",
    "        if ans == \"Y\":\n",
    "            return False\n",
    "        else:\n",
    "            print(\"Canceled....\")\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Function(Use)\n",
    "\n",
    "# Test for checking ICU_id missing in Lab_1103_csv\n",
    "def getMissingIDinLab(Lab_file, show=True):\n",
    "    test = sorted(Lab_file.ICU_id.unique())\n",
    "    s = 0\n",
    "    error_list = list()\n",
    "    for i in test:\n",
    "        #         print(i)\n",
    "        s += 1\n",
    "        if s != i:\n",
    "            if show:\n",
    "                print(f\"error! : {s}\")\n",
    "            error_list.append(s)\n",
    "            s += 1\n",
    "    if show:\n",
    "        print(f\"Missing ID Result: {error_list}\")\n",
    "    if show:\n",
    "        print(f\"Missing Length:{len(error_list)}\")\n",
    "    return error_list\n",
    "\n",
    "# store Dataframe to CSV\n",
    "\n",
    "\n",
    "def store2CSV(data, target_name, target_loc_prefix='./'):\n",
    "    file_path = target_loc_prefix+target_name+\".csv\"\n",
    "    if checkDuplicateFile(file_path):\n",
    "        print(\"store2CSV failed\")\n",
    "        return\n",
    "    data.to_csv(file_path)\n",
    "    print(\"store2CSV Successful!\")\n",
    "\n",
    "# store Datastruc. to pickle\n",
    "\n",
    "\n",
    "def store2Pickle(data, target_name, target_loc_prefix='./'):\n",
    "    import pickle\n",
    "    file_path = target_loc_prefix+target_name+'.pickle'\n",
    "    if checkDuplicateFile(file_path):\n",
    "        print(\"store2Pickle failed\")\n",
    "        return\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(\"store2Pickle Successful!\")\n",
    "\n",
    "\n",
    "def readFPickle(target_name, target_loc_prefix='./'):\n",
    "    import pickle\n",
    "    file_name = target_loc_prefix+target_name+'.pickle'\n",
    "    with open(file_name, 'rb') as f:\n",
    "        temp = pickle.load(f)\n",
    "    return temp\n",
    "\n",
    "# Function(Data preprocessing)\n",
    "\n",
    "#Function: 補值\n",
    "# 將針對輸入的df_data直接進行inplace插補\n",
    "# 須確保df_data的缺失值位置有放np.nan\n",
    "\n",
    "\n",
    "def handleMissing(df_data, df_feature, outFeature=[\"outcome\"], cate_astype=\"int\"):\n",
    "    for featureName in df_data.columns:\n",
    "        if featureName not in outFeature:\n",
    "            if df_data[featureName].isna().sum() == 0:\n",
    "                print(f\"{featureName}: Not need to fill.\")\n",
    "                continue\n",
    "            else:\n",
    "                # 先去看是連續與否 (1代表連續,0代表離散)\n",
    "                kindValue = df_feature.loc[df_feature[\"features name\"]\n",
    "                                           == featureName, \"kind\"].values[0]\n",
    "                if kindValue == 1:\n",
    "                    # continuous\n",
    "                    # mean filling\n",
    "                    targetMean = df_data[featureName].mean()\n",
    "                    df_data[featureName].fillna(value=targetMean, inplace=True)\n",
    "                    print(f\"{featureName}: Fill, Continuous.\")\n",
    "\n",
    "                else:\n",
    "                    # categorical\n",
    "                    # mode filling\n",
    "                    targetMode = df_data[featureName].mode()[0]\n",
    "                    df_data[featureName].fillna(value=targetMode, inplace=True)\n",
    "                    df_data[featureName] = df_data[featureName].astype(\n",
    "                        cate_astype)\n",
    "                    print(\n",
    "                        f\"{featureName}: Fill, Categorical. (astype to {cate_astype})\")\n",
    "\n",
    "    print(\"---handleMissing Finish---\")\n",
    "\n",
    "# plot hist\n",
    "# filtered_data need to check not have nan\n",
    "\n",
    "\n",
    "def plotHist(df_data, target, outcome=\"outcome\", bins=20):\n",
    "    filtered_data = pd.concat([df_data[\"outcome\"], df_data[target]], axis=1)\n",
    "    filtered_data = filtered_data.dropna()\n",
    "    print(filtered_data.isna().sum())\n",
    "    plt.hist(filtered_data.loc[filtered_data[outcome]\n",
    "             == 0, target], bins=bins, alpha=0.5, label='0')\n",
    "    plt.hist(filtered_data.loc[filtered_data[outcome]\n",
    "             == 1, target], bins=bins, alpha=0.5, label='1')\n",
    "    plt.xlabel(target)\n",
    "    plt.ylabel('count')\n",
    "    plt.legend(title=outcome)\n",
    "\n",
    "# plot countplot\n",
    "# filtered_data need to check not have nan\n",
    "\n",
    "\n",
    "def plotCountplot(df_data, target, outcome=\"outcome\"):\n",
    "    filtered_data = pd.concat([df_data[\"outcome\"], df_data[target]], axis=1)\n",
    "    filtered_data = filtered_data.dropna()\n",
    "    print(filtered_data.isna().sum())\n",
    "    sns.countplot(x=target, hue=outcome, data=filtered_data)\n",
    "\n",
    "# plot boxplot\n",
    "# filtered_data need to check not have nan\n",
    "\n",
    "\n",
    "def plotBoxplot(df_data, target, outcome=\"outcome\"):\n",
    "    filtered_data = pd.concat([df_data[\"outcome\"], df_data[target]], axis=1)\n",
    "    filtered_data = filtered_data.dropna()\n",
    "    print(filtered_data.isna().sum())\n",
    "    sns.boxplot(x=filtered_data[target], data=filtered_data)\n",
    "\n",
    "# Function(Model)\n",
    "\n",
    "\n",
    "def serializeModel(model, modelName, featureNum):\n",
    "    initial_type = [('float_input', FloatTensorType([None, featureNum]))]\n",
    "    onx = convert_sklearn(model, initial_types=initial_type)\n",
    "    with open(modelName + \".onnx\", \"wb\") as f:\n",
    "        f.write(onx.SerializeToString())\n",
    "\n",
    "\n",
    "def modelPredict(modelName, testData):\n",
    "    sess = rt.InferenceSession(modelName + '.onnx')  # load the onnx\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[0].name\n",
    "    pred_onx = sess.run([label_name], {input_name: testData.astype(np.float32)})[\n",
    "        0]  # predict testData\n",
    "    print(pred_onx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2959e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, y):\n",
    "    X = torch.from_numpy(X).float()#transform the data from numpy to torch\n",
    "    y = torch.from_numpy(y).long()\n",
    "    \n",
    "    dataset = TensorDataset(X,y)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True,\n",
    "                                         pin_memory=True, num_workers=2)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConvNet(self, kernel_size->list, input_size, classes=2)\n",
    "def make(config, X_train, y_train, X_val, y_val):\n",
    "    # Make the data\n",
    "    # X_train, y_train, X_val, y_val is dataframe\n",
    "    # now to ndarray\n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.values\n",
    "    X_val = X_val.values\n",
    "    y_val = y_val.values\n",
    "    train, test = get_data(X_train, y_train), get_data(X_val, y_val)\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    model = ConvNet(config.kernel_size, config.feature_size ,config.classes).to(device)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
    "\n",
    "    return model, train_loader, test_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters, X_train, y_train, X_val, y_val):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"DataMining_Project2\", entity=\"oscarchencs10\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, test_loader, criterion, optimizer = make(config, X_train, y_train, X_val, y_val)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        train(model, train_loader, test_loader,criterion, optimizer, config)\n",
    "\n",
    "        # and test its final performance\n",
    "        test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2270c",
   "metadata": {},
   "source": [
    "# Train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54deb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"acc\": correct / total, \"f1\":f1Score, \"auc\":auc}\n",
    "def train_log(loss, example_ct, epoch, metrics, batch_ct):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss, \"training_acc\":metrics[\"acc\"], \n",
    "               \"training_f1\":metrics[\"f1\"], \"training_auc\":metrics[\"auc\"]}, step=example_ct)\n",
    "    print(f\"----Epoch[{epoch}],Batch[{batch_ct}]----\")\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n",
    "    print(f\"Acc after \" + str(example_ct).zfill(5) + f\" examples: {metrics[\"acc\"]:.4f}\")\n",
    "    print(f\"f1_score after \" + str(example_ct).zfill(5) + f\" examples: {metrics[\"f1\"]:.4f}\")\n",
    "    print(f\"AUC after \" + str(example_ct).zfill(5) + f\" examples: {metrics[\"auc\"]:.4f}\")\n",
    "    print(f\"<-------------------------------------->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cc3ea",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b59103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conventional and convolutional neural network\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, kernel_size, input_size, classes=2):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=6, kernel_size=kernel_size[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=1))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=6, out_channels=16, kernel_size=kernel_size[1]),\n",
    "            nn.ReLU(),)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(16*(input_size-kernel_size[-1]+1), 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        x = torch.reshape(x, (-1,1, 40))\n",
    "#         print(x.shape)\n",
    "        out = self.layer1(x)\n",
    "#         print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        out = self.flatten(out)\n",
    "#         print(out.shape)\n",
    "        out = self.fc(out)\n",
    "#         print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55814ea1",
   "metadata": {},
   "source": [
    "# Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\"acc\": correct / total, \"f1\":f1_score, \"auc\":auc}\n",
    "def train(model, loader, test_loader, criterion, optimizer, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for _, (data_row, labels) in enumerate(loader):\n",
    "\n",
    "            loss = train_batch(data_row, labels, model, optimizer, criterion)\n",
    "            example_ct +=  len(data_row)\n",
    "            batch_ct += 1\n",
    "\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                metrics = test(model, test_loader, training_phase=True)\n",
    "                train_log(loss, example_ct, epoch, metrics, batch_ct)\n",
    "\n",
    "\n",
    "def train_batch(data_row, labels, model, optimizer, criterion):\n",
    "    data_row, labels = data_row.to(device), labels.to(device)\n",
    "    model.train()\n",
    "    # Forward pass ➡\n",
    "    outputs = model(data_row)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22753c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, training_phase=False):\n",
    "    model.eval()\n",
    "\n",
    "    # Run the model on some test examples\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        prob_all = []\n",
    "        prob_all2 = []\n",
    "        label_all = []\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1) #predicted:預測標籤結果\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            prob_all2.extend(outputs[:,1].cpu().numpy())\n",
    "            prob_all.extend(predicted.cpu().numpy())\n",
    "            label_all.extend(labels.cpu().numpy())\n",
    "\n",
    "        f1Score = f1_score(label_all, prob_all)\n",
    "        auc = roc_auc_score(label_all,prob_all2)\n",
    "        print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"test data: {100 * correct / total}%\")\n",
    "        print(f\"F1-Score of the model on the {total} \" +\n",
    "              \"test data:{:.4f}\".format(f1Score))\n",
    "        print(f\"AUC of the model on the {total} \" +\n",
    "              \"test data:{:.4f}\".format(auc))\n",
    "        \n",
    "        if not training_phase:\n",
    "            wandb.log({\"test_accuracy\": correct / total, \"test_f1\":f1Score, \"test_auc\":auc})\n",
    "            return None\n",
    "        else:\n",
    "            return {\"acc\": correct / total, \"f1\":f1Score, \"auc\":auc}\n",
    "\n",
    "#     # Save the model in the exchangeable ONNX format\n",
    "#     torch.onnx.export(model, images, \"model.onnx\")\n",
    "#     wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575df80a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e0dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp13_X_train = readFPickle(\"data/exp13/exp13_X_train\")\n",
    "exp13_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb38fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp13_y_train = readFPickle(\"data/exp13/exp13_y_train\")\n",
    "exp13_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa6164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp13_X_val = readFPickle(\"data/exp13/exp13_X_val\")\n",
    "exp13_X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23896fb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp13_y_val = readFPickle(\"data/exp13/exp13_y_val\")\n",
    "exp13_y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c7bc6",
   "metadata": {},
   "source": [
    "# Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92308d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1804b4",
   "metadata": {},
   "source": [
    "# GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85434356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764a8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84956936",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f26d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb56535",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=100,\n",
    "    classes=2,\n",
    "    kernel_size=[1, 1],\n",
    "    feature_size=40,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.005,\n",
    "    dataset=\"PJI\",\n",
    "    architecture=\"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d718a1c",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa485d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build, train and analyze the model with the pipeline\n",
    "model = model_pipeline(config, exp13_X_train, exp13_y_train, exp13_X_val, exp13_y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "313px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "609.844px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
